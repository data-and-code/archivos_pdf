---
title: "Archivos PDF"
output:
  rmdformats::readthedown:
    highlight: tango
    cards: false
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
* Hecho por:

  + Calvin Alberto López Alvarez

  + Fernando Gomez Perera

  + Ricardo Sinaí Vargas Kumul
  
***

# Introducción
En estos proyectos usaremos diferentes herramientas y metodologías como: [programación funcional](https://www.ionos.mx/digitalguide/paginas-web/desarrollo-web/programacion-funcional/), 
[programacion paralela](https://www.teldat.com/blog/es/computacion-paralela-capacidad-procesamiento/), [web scrapping](https://aukera.es/blog/web-scraping/), [lógica difusa](https://www.sciencedirect.com/science/article/pii/S0186104217300955), [procesamiento del lenguaje natural](https://towardsdatascience.com/your-guide-to-natural-language-processing-nlp-48ea2511f6e1), [expresiones regulares](https://www.ionos.mx/digitalguide/paginas-web/creacion-de-paginas-web/regex/), y más....

Todo este documento, la app y demás materiales al igual que otros proyectos se encuentran disponibles para su consulta y descarga en [Data&Code.com](https://data-and-code.github.io)

***

# Proyecto 1

En este primer proyecto tenemos varios objetivos a cumplir, y estos son:

1. Extraer la información que se encuentre en la [primera base de datos](https://drive.google.com/drive/u/0/folders/1CUSIWYEE9MGIOuoFqAH3UBRIq_m-2b5D).
2. Crear una tabla con el número de palabras totales por ciclo.
3. Crear  una tabla con las 10 palabras con mayor frecuencia que coinciden por ciclo.
4. Crear una tabla con las palabras que son únicas en cada archivo.
5. Crear una tabla con la siguiente información: nombre de la asignatura, clave, ciclo, créditos.
6. Construir una salida, donde se pueda exportar la tabla del paso 5 en formatos: .csv, .xls, .doc, .txt y html.
7. Extra: crear un grafo ideal para que un estudiante regular termine su carrera (contar los posibles caminos). 
8. Redactar conclusiones del trabajo realizado.

## Bibliotecas de R a usar
[Pacman](http://https://www.rdocumentation.org/packages/pacman/versions/0.5.1) envuelve convenientemente las funciones relacionadas con la biblioteca y el paquete y las nombra de una manera intuitiva y consistente.

~~Basicamente, nos permite instalar lo que nos hace falta de las librerias para su uso(incluso la propia libreria) y llamarla al mismo tiempo.~~

```{r}
pacman::p_load(fs, magrittr, purrr, tibble, dplyr, stringr, quanteda, pdftools, DT)
```

## 1. Extraer la información que se encuentre en la primera base de datos.
* Aquí lo que hacemos es :
  + Leer el contenido de los archivos PDFs y almacenarlos en una sola lista.
  + Extraer los nombres de los ciclos en orden.
  + Obtener una lista de listas con el contenido de los PDFs agrupados por ciclos.
    - Separar y agrupar los PDFs por ciclos.
    - Asignar el nombre de cada ciclo.
    - Renombrar los PDFs extrayendo solamente el nombre de la materia.
```{r}

pdf_files <- dir_ls("Temarios IDeIO finales/", recurse = TRUE, glob = "*.pdf") %>%
  map(pdf_text) %>%
  map_chr(str_c, collapse = " ")

cycles <- names(pdf_files) %>%
  str_split_fixed(pattern = "/", n = 3) %>%
  extract(,2) %>%
  unique() %>%
  extract(c(2:4, 1))

syllabus <- cycles %>%
  map(~ keep(pdf_files, str_detect(names(pdf_files), pattern = .x))) %>%
  set_names(nm = cycles) %>%
  map_depth(1, ~ set_names(.x, str_extract(names(.x), pattern = "[A-Z]{2}[0-9]{3}.*(?=.pdf)")))

```

### Visualizamos el resultado 1
```{r}
str(syllabus)
```

## 2. Crear una tabla con el número de palabras totales por ciclo.

* Aquí lo que hacemos es :
  + Importar al entorno los terminos de parada (stop words) en español
  + Composicion de funciones para obtener las palabras del texto
    - Convertir el texto a minusculas
    - Dividir y extraer las subcadenas de texto que correspondan a palabras
    - Convertir el resultado a un vector de caracteres
    - Eliminar los terminos de parada de las palabras
    - Eliminar los tokens que solo estan compuestos por un caracter
    - Ejecutar las funciones en el orden que fueron declaradas
  + Extraer todas las palabras por temario
  + Obtener el numero de palabras totales por ciclo
    - Contar el numero de palabras de cada temario
    - Obtener el numero de palabras por ciclo
    - Convertir la lista a una tabla
```{r}
stop_words_es <- stopwords(language = "es")

get_words <- compose(
  str_to_lower,
  ~ tokens(.x, remove_punct = TRUE, remove_numbers = TRUE, remove_url = TRUE),
  as.character,
  ~ .x[!.x %in% stop_words_es],
  ~ .x[str_length(.x) > 1],
  .dir = "forward"
)

words_by_syllabus <- syllabus %>%
  map_depth(2, get_words)

num_words_by_cycle <- words_by_syllabus %>%
  map_depth(2, length) %>%
  map_depth(1, reduce, sum) %>%
  as_tibble()
```

### Visualizamos el resultado 2
```{r}
datatable(num_words_by_cycle)
```

## 3. Crear una tabla con las 10 palabras con mayor frecuencia que coinciden por ciclo.

* Eliminar un nivel para tener las palabras de todos los temarios por ciclo
  + Convertir los arreglos de palabras en una matriz dispersa de termino-documento por ciclo
  + Obtener las 10 palabras mas frecuentes por ciclo
  + Seleccionar ciertas columnas
  + Agregar una columna con el nombre del ciclo
  + Convertir el resultado en una sola tabla
```{r}
most_freq_by_cycle <- words_by_syllabus %>%
  map_depth(1, ~ reduce(.x, append)) %>%
  map(dfm) %>%
  map(textstat_frequency, n = 10) %>%
  map(as_tibble) %>%
  map(~ select(.x, Palabras = feature, Frecuencia = docfreq)) %>%
  map2(cycles, ~ add_column(.x, Ciclo = .y)) %>%
  reduce(bind_rows)
```

### Visualizamos el resultado 3
```{r}
datatable(most_freq_by_cycle)
```

## 4. Crear una tabla con las palabras que son únicas en cada archivo.

* Tener las palabras por temario eliminando la agrupacion por ciclo
* Obtener las palabras que son unicas en cada archivo
  + Filtrar las palabras para que sean unicas
  + Agregar una columna con el nombre del ciclo
  + Convertir el resultado en una sola tabla
  + Renombrar la columna con las palabras
  
```{r}

words_by_syllabus_no_cycle <- words_by_syllabus %>%
  flatten()

uniq_words_by_syllabus <- words_by_syllabus_no_cycle %>%
  map2(names(words_by_syllabus_no_cycle), ~ .x[!.x %in% reduce(words_by_syllabus_no_cycle[names(words_by_syllabus_no_cycle) != .y], append)]) %>%
  map(unique) %>%
  map(as_tibble) %>%
  map2(names(words_by_syllabus_no_cycle), ~ add_column(.x, Archivo = .y)) %>%
  reduce(bind_rows) %>%
  rename(Palabras = value)
```

# Visualizamos el resultado 4

```{r}
datatable(uniq_words_by_syllabus)
```

## 5. Crear una tabla con la siguiente información: nombre de la asignatura, clave, ciclo, créditos.

* Extraer la informacion con expresiones regulares
  + Reducir las dimensiones para eliminar la agrupacion por ciclos
  + Convertir el resultado a una tabla
  
```{r}
subjects_info <-
  list(
    "Nombre de la asignatura" = map_depth(syllabus, 1, ~ str_trim(str_extract(names(.x), pattern = "(?<=[A-Z]{2}[0-9]{4}).*"))),
    "Clave" = map_depth(syllabus, 1, ~ str_extract(names(.x), pattern = "[A-Z]{2}[0-9]{4}")),
    "Ciclo" = map_depth(syllabus, 1, ~ str_extract(.x, pattern = "(?<=Ãrea de formaciÃ³n curricular\n\\s?)[1-4]-[1-4]")),
    "CrÃ©ditos" = map_depth(syllabus, 1, ~ str_trim(str_extract(.x, pattern = "(?<=[A-Z]{2}[0-9]{4}\\s{2,10}).*(?=\\s{2,10}(Profesional Asociado|Licenciatura (BÃ¡sica|ElecciÃ³n|Preespecialidad)))")))
  ) %>%
  map_depth(1, ~ reduce(.x, append)) %>%
  as_tibble()
```

# Visualizamos el resultado 5

```{r}
datatable(subjects_info)
```

## 6. Construir una salida, donde se pueda exportar la tabla del paso 5 en formatos: .csv, .xls, .doc, .txt y html.

## 7. Extra: crear un grafo ideal para que un estudiante regular termine su carrera (contar los posibles caminos). 

## 8. Redactar conclusiones del trabajo realizado.

Después de estár probando todas estas técnicas y herramientas para la extracción de datos que mencionamos en la introducción, podemos ver el potencial de poder trabajar con casi cualquier fuente de datos que tengamos **o no** disponibles, y a que hemos podido *rascar* datos de PDFs, Imagenes, demás, mientras que aprovechamos las habilidades que ya poseemos referente a visualización de datos como shiny, RMD, etc. que obtuvimos de la materia Analisís de Grandes Volumenes de Datos.

Si bien aun nos queda aspectos que pulir del lado de ETL como de visualización, iremos liberando las actualizaciones conforme logremos puntos importantes.

***

# Proyecto 2

En este segundo proyecto tenemos varios objetivos a cumplir, y estos son:

1. Extraer la información que se encuentre en la [segunda base de datos](https://app.schoology.com/link?path=https%3A%2F%2Fwww.gob.mx%2Fsalud%2Facciones-y-programas%2Fcoronavirus-covid-19-comunicados-tecnicos-diarios-historicos-2020). (Cada grupo debe trabajar un mes diferente). Los meses a trabajar son: mayo-diciembre del 2020.
2. Con la información proporcionada a nivel mundial, cree un tabla y muestre un gráfico que visualice la información mensual.
3. Con la información de defunciones positivas, cree un tabla y muestre un gráfico que visualice la información mensual.
4. Redactar conclusiones del trabajo realizado.

#Extraemos los datos

Este codigo no se ejecutara debido al peso computancial alto que implica; Por otro lado, ya esta precargado los datos obtenidos por el código y será con lo que trabajaremos.

De cualquier forma, el código se mantiene a la vista para la comprensión del procedimiento.

```{r eval=FALSE, include=FALSE}
# Extraccion del texto de los PDFs
pacman::p_load(pdftools, tesseract, rvest, purrr, furrr, stringr)

# Definir el numero de workers que permitiran ejecutar funciones de purrr de forma paralela
plan(multisession, workers = 4)

# Obtener las direcciones de los PDFs diarios del mes (Septiembre)
pdf_urls <- read_html("https://www.gob.mx/salud/documentos/coronavirus-covid-19-comunicados-tecnicos-diarios-septiembre-2020") %>%
  # Obtener los nodos HTML que corresponden a la etiqueta "a"
  html_nodes(css = "a") %>%
  # Extraer de los nodos el valor dentro del atributo "href", el cual contiene URLs
  html_attr(name = "href") %>%
  # Descartar los valores NA's
  discard(is.na) %>%
  # Quedarse con las URLS que terminen con ".pdf"
  keep(str_detect, pattern = ".pdf$") %>%
  # Completar cada URL
  map_chr(~ str_c("https://www.gob.mx", .x))

# Extraer el texto de cada PDF utilizando distintos valores de dpi
pdf_files_1000 <- pdf_urls %>%
  # Usar Tesseract para detectar el texto dentro de cada PDF (incluso del texto dentro de las imagenes)
  future_map(pdf_ocr_text, pages = c(1, 5), language = "spa", dpi = 1000) %>%
  # Renombrar cada elemento del resultado con su nombre de archivo original
  set_names(nm = str_extract(pdf_urls, pattern = "(?<=/)Comunicado.*\\.pdf$")) %>%
  # Invertir el orden del resultado para que los documentos se ordenen por fecha de forma ascendente
  rev()

pdf_files_800 <- pdf_urls %>%
  # Usar Tesseract para detectar el texto dentro de cada PDF (incluso del texto dentro de las imagenes)
  future_map(pdf_ocr_text, pages = c(1, 5), language = "spa", dpi = 800) %>%
  # Renombrar cada elemento del resultado con su nombre de archivo original
  set_names(nm = str_extract(pdf_urls, pattern = "(?<=/)Comunicado.*\\.pdf$")) %>%
  # Invertir el orden del resultado para que los documentos se ordenen por fecha de forma ascendente
  rev()

pdf_files_750 <- pdf_urls %>%
  # Usar Tesseract para detectar el texto dentro de cada PDF (incluso del texto dentro de las imagenes)
  future_map(pdf_ocr_text, pages = c(1, 5), language = "spa", dpi = 750) %>%
  # Renombrar cada elemento del resultado con su nombre de archivo original
  set_names(nm = str_extract(pdf_urls, pattern = "(?<=/)Comunicado.*\\.pdf$")) %>%
  # Invertir el orden del resultado para que los documentos se ordenen por fecha de forma ascendente
  rev()

pdf_files_700 <- pdf_urls %>%
  # Usar Tesseract para detectar el texto dentro de cada PDF (incluso del texto dentro de las imagenes)
  future_map(pdf_ocr_text, pages = c(1, 5), language = "spa", dpi = 700) %>%
  # Renombrar cada elemento del resultado con su nombre de archivo original
  set_names(nm = str_extract(pdf_urls, pattern = "(?<=/)Comunicado.*\\.pdf$")) %>%
  # Invertir el orden del resultado para que los documentos se ordenen por fecha de forma ascendente
  rev()

pdf_files_650 <- pdf_urls %>%
  # Usar Tesseract para detectar el texto dentro de cada PDF (incluso del texto dentro de las imagenes)
  future_map(pdf_ocr_text, pages = c(1, 5), language = "spa", dpi = 650) %>%
  # Renombrar cada elemento del resultado con su nombre de archivo original
  set_names(nm = str_extract(pdf_urls, pattern = "(?<=/)Comunicado.*\\.pdf$")) %>%
  # Invertir el orden del resultado para que los documentos se ordenen por fecha de forma ascendente
  rev()

pdf_files_600 <- pdf_urls %>%
  # Usar Tesseract para detectar el texto dentro de cada PDF (incluso del texto dentro de las imagenes)
  future_map(pdf_ocr_text, pages = c(1, 5), language = "spa", dpi = 600) %>%
  # Renombrar cada elemento del resultado con su nombre de archivo original
  set_names(nm = str_extract(pdf_urls, pattern = "(?<=/)Comunicado.*\\.pdf$")) %>%
  # Invertir el orden del resultado para que los documentos se ordenen por fecha de forma ascendente
  rev()

pdf_files_550 <- pdf_urls %>%
  # Usar Tesseract para detectar el texto dentro de cada PDF (incluso del texto dentro de las imagenes)
  future_map(pdf_ocr_text, pages = c(1, 5), language = "spa", dpi = 550) %>%
  # Renombrar cada elemento del resultado con su nombre de archivo original
  set_names(nm = str_extract(pdf_urls, pattern = "(?<=/)Comunicado.*\\.pdf$")) %>%
  # Invertir el orden del resultado para que los documentos se ordenen por fecha de forma ascendente
  rev()

pdf_files_500 <- pdf_urls %>%
  # Usar Tesseract para detectar el texto dentro de cada PDF (incluso del texto dentro de las imagenes)
  future_map(pdf_ocr_text, pages = c(1, 5), language = "spa", dpi = 500) %>%
  # Renombrar cada elemento del resultado con su nombre de archivo original
  set_names(nm = str_extract(pdf_urls, pattern = "(?<=/)Comunicado.*\\.pdf$")) %>%
  # Invertir el orden del resultado para que los documentos se ordenen por fecha de forma ascendente
  rev()

# Almacenar los resultados en una sola lista
pdf_files <- list(
  dpi_500 = pdf_files_500,
  dpi_550 = pdf_files_550,
  dpi_600 = pdf_files_600,
  dpi_650 = pdf_files_650,
  dpi_700 = pdf_files_700,
  dpi_750 = pdf_files_750,
  dpi_800 = pdf_files_800,
  dpi_1000 = pdf_files_1000
)

# Se almacena el resultado en un archivo RData para usarlo el resto del proyecto
save(pdf_files, file = "base_de_datos_2.RData")

```

## 2. Con la información proporcionada a nivel mundial, cree un tabla y muestre un gráfico que visualice la información mensual.

## 3. Con la información de defunciones positivas, cree un tabla y muestre un gráfico que visualice la información mensual.

## 4. Redactar conclusiones del trabajo realizado.
